{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#import tensorflow as tf\n",
    "#from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from tensorflow.keras.callbacks import EarlyStopping\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "\n",
    "wrangleData = True\n",
    "processCardText = True\n",
    "loadInventoryFromFile = True\n",
    "reduceDataSetSize = True\n",
    "createHoldoutData = True\n",
    "performDataExploration = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Data Wrangling & Database Consolidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Transform competitor inventory data into a structured DataFrame.\n",
    "Arguments:\n",
    "    theCompetitorData (DataFrame): Input DataFrame with 'history' column.\n",
    "    maxNumberOfRows (int, optional): Limits number of rows to process. Defaults to all rows.\n",
    "Returns:\n",
    "    DataFrame: Transformed inventory data.\n",
    "\"\"\"\n",
    "def createDataFrameFromInventories(theCompetitorData, maxNumberOfRows=None):\n",
    "    rows = []\n",
    "    price_memory = {}\n",
    "\n",
    "    # Determine row processing limit\n",
    "    total_rows = len(theCompetitorData)\n",
    "    rows_to_process = min(total_rows, maxNumberOfRows) if maxNumberOfRows is not None else total_rows\n",
    "    progress_step = max(rows_to_process * 2 // 100, 1)\n",
    "\n",
    "    for i in range(rows_to_process):\n",
    "        row = theCompetitorData.iloc[i]\n",
    "        history_entries = row['history'].split('|')\n",
    "        key = (row['competitorName'], row['cardNumber'])\n",
    "\n",
    "        for entry in history_entries:\n",
    "            try:\n",
    "                price, qty, date = entry.split('*')\n",
    "\n",
    "                if price.strip().lower() == 'sold out':\n",
    "                    if key in price_memory:\n",
    "                        clean_price = price_memory[key]\n",
    "                    else:\n",
    "                        continue\n",
    "                else:\n",
    "                    price = price.replace(',', '').strip()\n",
    "                    clean_price = float(price)\n",
    "                    price_memory[key] = clean_price\n",
    "\n",
    "                clean_qty = int(qty) if qty.isdigit() else 0\n",
    "                rows.append({\n",
    "                    'competitorName': row['competitorName'],\n",
    "                    'cardNumber': row['cardNumber'],\n",
    "                    'price': clean_price,\n",
    "                    'qty': clean_qty,\n",
    "                    'history': date\n",
    "                })\n",
    "            except ValueError as e:\n",
    "                print(f\"Skipping card #{row['cardNumber']}, malformed entry: {entry} â†’ {e} ({history_entries})\")\n",
    "        \n",
    "        # Show progress\n",
    "        if (i + 1) % progress_step == 0 or i + 1 == rows_to_process:\n",
    "            percent = ((i + 1) / rows_to_process) * 100\n",
    "            print(f\"Processed {i + 1}/{rows_to_process} rows ({percent:.1f}%)\")\n",
    "\n",
    "    inventoryData = pd.DataFrame(rows)\n",
    "    return inventoryData\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!!!: Loading data files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\consu\\AppData\\Local\\Temp\\ipykernel_16428\\2912120789.py:4: DtypeWarning: Columns (76,79) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  theCardData = pd.read_csv('data/magic_cards.csv', encoding='utf-8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!!!: Wrangling card data\n",
      "!!!!!: Merging card and edition data\n",
      "!!!!!: Loading inventory data file\n",
      "!!!!!: Performing final merge\n",
      "!!!!!: Performing card age calculations\n",
      "!!!!!: Saving final data to a file\n"
     ]
    }
   ],
   "source": [
    "if wrangleData == True:\n",
    "    # Load directly into a DataFrame\n",
    "    print(f\"!!!!!: Loading data files\")\n",
    "    theCardData = pd.read_csv('data/magic_cards.csv', encoding='utf-8')\n",
    "    theEditionData = pd.read_csv('data/editions.csv', encoding='utf-8')\n",
    "    theCompetitorData = pd.read_csv('data/competitorInventories.csv', encoding='utf-8')\n",
    "\n",
    "    print(f\"!!!!!: Wrangling card data\")\n",
    "    # Filter on specific rows\n",
    "    cardData = theCardData[\n",
    "        (theCardData['status'] == 'Active') & # Only non-deleted inventory\n",
    "        (theCardData['type'] == 'Normal') & # Non-foils\n",
    "        (theCardData['true_rarity'] != 'Basic Land') & # Basic Lands prices rarely change ovr time, including them would skew the data\n",
    "        (theCardData['true_rarity'] != 'Common') & # Commons are in too great abundance to yield the greatest returns on the investment\n",
    "        (theCardData['2nd_converted_casting_cost'] == 0) & # Remove dual cards as there aren't enough of them and they could skew results.\n",
    "        (~theCardData['spelltype'].astype(str).str.startswith('Planeswalker')) & # Remove all planeswalkers from the data set\n",
    "        (theCardData['language_number'] == 1) # Only english versions\n",
    "    ]\n",
    "\n",
    "    editionData = theEditionData[\n",
    "        (theEditionData['status'] == 'Active') # Only non-deleted editions\n",
    "    ]\n",
    "\n",
    "    editionData = editionData[['number', 'release_date']]\n",
    "\n",
    "    # Remove fields associated with dual cards\n",
    "    cardData = cardData.drop(['2nd_name', '2nd_casting_b', '2nd_casting_u', '2nd_casting_g', '2nd_casting_r', '2nd_casting_w', '2nd_casting_bu', '2nd_casting_bg', '2nd_casting_br', '2nd_casting_bw', '2nd_casting_ug', '2nd_casting_ur', '2nd_casting_uw', '2nd_casting_gr', '2nd_casting_gw', '2nd_casting_rw', '2nd_casting_2b', '2nd_casting_2u', '2nd_casting_2g', '2nd_casting_2r', '2nd_casting_2w', '2nd_casting_bp', '2nd_casting_up', '2nd_casting_gp', '2nd_casting_rp', '2nd_casting_wp', '2nd_casting_colorless', '2nd_casting_x', '2nd_converted_casting_cost', '2nd_color', '2nd_spelltype', '2nd_power', '2nd_toughness', '2nd_text', '2nd_flavortext', '2nd_artist_number'], axis=1)\n",
    "\n",
    "    # Remove fields with no real value\n",
    "    cardData = cardData.drop(['artist_number', 'flavortext', 'last_modification_date', 'publisher_id', 'status', 'is_always_foil', 'weight', 'image_numbers', 'language_number', 'price', 'type'], axis=1)\n",
    "\n",
    "    # Clean up various NaN values\n",
    "    cardData[['power', 'toughness', 'casting_2b', 'casting_2u', 'casting_2g', 'casting_2r', 'casting_2w', 'casting_bp', 'casting_up', 'casting_gp', 'casting_rp', 'casting_wp']] = cardData[['power', 'toughness', 'casting_2b', 'casting_2u', 'casting_2g', 'casting_2r', 'casting_2w', 'casting_bp', 'casting_up', 'casting_gp', 'casting_rp', 'casting_wp']].fillna(0)\n",
    "    cardData['color'] = cardData['color'].fillna('Unknown')\n",
    "    cardData['spelltype'] = cardData['spelltype'].fillna('Unknown')\n",
    "    \n",
    "    # Recalculate converted casting cost after NaN cleanup\n",
    "    cardData['converted_casting_cost'] = cardData[\n",
    "        [\"casting_b\", \"casting_u\", \"casting_g\", \"casting_r\", \"casting_w\",\n",
    "        \"casting_bu\", \"casting_bg\", \"casting_br\", \"casting_bw\",\n",
    "        \"casting_ug\", \"casting_ur\", \"casting_uw\",\n",
    "        \"casting_gr\", \"casting_gw\", \"casting_rw\"]\n",
    "    ].sum(axis=1)\n",
    "\n",
    "    # Fix a couple of issues with the data where the cards don't have a true_rarity set\n",
    "    #cardData.loc[cardData['name'].isin(['Platinum Angel', 'Bogardan Hellkite']), 'true_rarity'] = 'Mythic Rare'\n",
    "    #print(cardData['true_rarity'].unique())\n",
    "\n",
    "    # Convert rarity to a frequency\n",
    "    mythicEditions = {60, 15, 1, 78, 79, 34, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 108, 109, 123, 128, 143, 144, 145, 146, 156, 157}\n",
    "    isMythic = cardData[\"edition_number\"].isin(mythicEditions)\n",
    "    cardData.loc[isMythic, \"frequency\"] = np.select(\n",
    "        [\n",
    "            cardData.loc[isMythic, \"true_rarity\"] == \"Common\",\n",
    "            cardData.loc[isMythic, \"true_rarity\"] == \"Uncommon\",\n",
    "            cardData.loc[isMythic, \"true_rarity\"] == \"Rare\",\n",
    "            cardData.loc[isMythic, \"true_rarity\"] == \"Mythic Rare\"\n",
    "        ],\n",
    "        [792/1080, 216/1080, 63/1080, 9/1080],\n",
    "        default=np.nan\n",
    "    )\n",
    "    cardData.loc[~isMythic, \"frequency\"] = np.select(\n",
    "        [\n",
    "            cardData.loc[~isMythic, \"true_rarity\"] == \"Common\",\n",
    "            cardData.loc[~isMythic, \"true_rarity\"] == \"Uncommon\",\n",
    "            cardData.loc[~isMythic, \"true_rarity\"] == \"Rare\"\n",
    "        ],\n",
    "        [11/15, 3/15, 1/15],\n",
    "        default=np.nan\n",
    "    )\n",
    "\n",
    "    # Add columns based upon card text\n",
    "    if processCardText == True:\n",
    "        mtgAbilities = [\"Affinity\", \"Aftermath\", \"Adventure\", \"Cascade\", \"Companion\", \"Convoke\", \"Cycling\", \"Cumulative Upkeep\", \"Deathtouch\", \"Delve\", \"Defender\", \"Double Strike\", \"Echo\", \"Enchant\", \"Encore\", \"Equip\", \"Escalate\", \"Escape\", \"Exalted\", \"Exploit\", \"Evolve\", \"Extort\", \"First Strike\", \"Flash\", \"Flashback\", \"Flanking\", \"Flying\", \"Foretell\", \"Haste\", \"Hexproof\", \"Indestructible\", \"Infect\", \"Intimidate\", \"Landwalk\", \"Lifelink\", \"Megamorph\", \"Menace\", \"Miracle\", \"Morph\", \"Mutate\", \"Persist\", \"Phasing\", \"Protection\", \"Prowess\", \"Rampage\", \"Reach\", \"Scry\", \"Shadow\", \"Shroud\", \"Split Second\", \"Suspend\", \"Toxic\", \"Trample\", \"Undying\", \"Vigilance\"]\n",
    "        for ability in mtgAbilities:\n",
    "            col_name = f'ability{ability.replace(\" \", \"\")}'  # Remove spaces for column naming\n",
    "            pattern = rf'\\b{ability.lower()}\\b'              # Match whole word, case-insensitive\n",
    "            cardData[col_name] = cardData['text'].str.contains(pattern, case=False, na=False).astype(int)\n",
    "\n",
    "    # Remove text field after processing\n",
    "    cardData = cardData.drop(['text'], axis=1)\n",
    "\n",
    "    # Convert any instances of pwoer or tougness to a numeric value. These could skew results so I might want to omit those records altogether\n",
    "    cardData['power'] = cardData['power'].replace('*', 2.5)\n",
    "    cardData['toughness'] = cardData['toughness'].replace('*', 2.5)\n",
    "    \n",
    "    # Remove any records where the power or toughness are not numeric.\n",
    "    cardData['power_numeric'] = pd.to_numeric(cardData['power'], errors='coerce')\n",
    "    cardData['toughness_numeric'] = pd.to_numeric(cardData['toughness'], errors='coerce')\n",
    "    cardData = cardData[cardData[['power_numeric', 'toughness_numeric']].notna().all(axis=1)]\n",
    "    cardData = cardData.drop(columns=['power_numeric', 'toughness_numeric'])\n",
    "    cardData['power'] = pd.to_numeric(cardData['power'], errors='coerce')\n",
    "    cardData['toughness'] = pd.to_numeric(cardData['toughness'], errors='coerce')\n",
    "\n",
    "    # Remove any instances where power or toughness are negative numbers. I don't think that there are any legitimate instances.\n",
    "    cardData = cardData[\n",
    "        (cardData['power'] >= 0) & \n",
    "        (cardData['power'] <= 10) & # Remove outliers\n",
    "        (cardData['toughness'] >= 0) &\n",
    "        (cardData['toughness'] <= 10) # Remove outliers\n",
    "    ]\n",
    "\n",
    "    # unique_values = cardData[\"power\"].dropna().unique().tolist()\n",
    "    # print(unique_values)\n",
    "\n",
    "    print(f\"!!!!!: Merging card and edition data\")\n",
    "    mergedData = pd.merge(\n",
    "        cardData, \n",
    "        editionData, \n",
    "        left_on='edition_number', \n",
    "        right_on='number', \n",
    "        how='inner'\n",
    "    )\n",
    "\n",
    "    mergedData = mergedData.drop(['number_y', 'true_rarity'], axis=1)\n",
    "\n",
    "    # # Get a list of editions that had mythic rares\n",
    "    # mergedData['release_date'] = pd.to_datetime(mergedData['release_date'], format='%Y%m%d', errors='coerce')\n",
    "    # filtered = mergedData[mergedData[\"release_date\"] >= datetime(2008, 10, 1)]\n",
    "    # unique_editions = filtered[\"edition_number\"].dropna().unique().tolist()\n",
    "    # print(unique_editions)\n",
    "    \n",
    "    #checkingData = mergedData[\n",
    "    #    (mergedData['abilityShadow'] == 1)\n",
    "    #    (mergedData['number_x'] == 21841)\n",
    "    #]\n",
    "\n",
    "    if loadInventoryFromFile:\n",
    "        print(f\"!!!!!: Loading inventory data file\")\n",
    "        inventoryData = pd.read_csv('data/cleanedInventoryData.csv', encoding='utf-8')\n",
    "    else:\n",
    "        print(f\"!!!!!: Wrangling inventory data and saving the results to a file\")\n",
    "        inventoryData = createDataFrameFromInventories(theCompetitorData)\n",
    "        inventoryData.to_csv('data/cleanedInventoryData.csv', index=False, encoding='utf-8')    \n",
    "\n",
    "    print(f\"!!!!!: Performing final merge\")\n",
    "    finalMergedData = pd.merge(\n",
    "        inventoryData, \n",
    "        mergedData, \n",
    "        left_on='cardNumber', \n",
    "        right_on='number_x', \n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    print(f\"!!!!!: Performing card age calculations\")\n",
    "    # Force conversion and coerce invalid entries to NaT\n",
    "    finalMergedData['release_date'] = pd.to_datetime(finalMergedData['release_date'], format='%Y%m%d', errors='coerce')\n",
    "    finalMergedData['history'] = pd.to_datetime(finalMergedData['history'], format='%Y%m%d', errors='coerce')\n",
    "    # Drop any records where the date is unknown as the data is suspect\n",
    "    finalMergedData.dropna(subset=['release_date', 'history'], inplace=True)\n",
    "    #finalMergedData['release_date'] = pd.to_datetime(finalMergedData['release_date'], format='%Y%m%d')\n",
    "    #finalMergedData['history'] = pd.to_datetime(finalMergedData['history'], format='%Y%m%d')\n",
    "    finalMergedData['daysSinceRelease'] = (finalMergedData['history'] - finalMergedData['release_date']).dt.days\n",
    "    originalGameDate = pd.Timestamp('1993-01-01')\n",
    "    finalMergedData['daysSinceGameRelease'] = (finalMergedData['history'] - originalGameDate).dt.days\n",
    "\n",
    "    finalMergedData = finalMergedData.drop(['number_x', 'cardNumber', 'history', 'edition_number', 'release_date'], axis=1)\n",
    "\n",
    "    finalMergedData = finalMergedData[\n",
    "        (finalMergedData['price']  < 20) &\n",
    "        (finalMergedData['price']  > 0.26) & \n",
    "        (finalMergedData['frequency']  > 0) # Only elements with a known frequency. There are a couple of promo cards that will skew the results\n",
    "    ]\n",
    "\n",
    "    # Save wrangled data to a file\n",
    "    print(f\"!!!!!: Saving final data to a file\")\n",
    "    finalMergedData.to_csv('data/wrangledData.csv', index=False, encoding='utf-8') \n",
    "\n",
    "    # missing_counts = finalMergedData.isna().sum()\n",
    "    # print(missing_counts[missing_counts > 0])\n",
    "    # print(cardData['true_rarity'].dropna().unique())\n",
    "    #print(cardData['true_rarity'].unique())\n",
    "\n",
    "\n",
    "    # List of remaining columns\n",
    "    #print(cardData.columns.tolist())\n",
    "    #print(editionData.columns.tolist())\n",
    "    #print(mergedData.columns.tolist())\n",
    "    #print(theCompetitorData.columns.tolist())\n",
    "\n",
    "    # View the first 5 rows\n",
    "    #print(cardData.head(5))\n",
    "    #print(mergedData.head(30))\n",
    "    #print(checkingData.head(5))\n",
    "    #print(inventoryData.head(20))\n",
    "    #print(finalMergedData.head(10))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate the data into training and holdout sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!!!: Loading the wrangled data\n",
      "!!!!!: Splitting off 10.0% of the data, otherwise the data set is too large to utilize.\n",
      "!!!!!: Loading the sampled data\n",
      "!!!!!: Splitting off a holdout set of size 20.0%. There are 442554 records.\n"
     ]
    }
   ],
   "source": [
    "if reduceDataSetSize:\n",
    "    print(f\"!!!!!: Loading the wrangled data\")\n",
    "    # Data set is too large, creating a smaller sampling of the data\n",
    "    theData = pd.read_csv(\"data/wrangledData.csv\")\n",
    "    sampleSize = 0.1\n",
    "    print(f\"!!!!!: Splitting off {sampleSize * 100}% of the data, otherwise the data set is too large to utilize.\")\n",
    "    sampledData = theData.sample(frac=sampleSize, random_state=42)\n",
    "    sampledData.to_csv(\"data/sampledData.csv\", index=False)\n",
    "\n",
    "if createHoldoutData:\n",
    "    print(f\"!!!!!: Loading the sampled data\")\n",
    "\n",
    "    holdoutFraction = 0.2\n",
    "    # Load the dataset\n",
    "    theData = pd.read_csv(\"data/sampledData.csv\")\n",
    "\n",
    "    print(f\"!!!!!: Splitting off a holdout set of size {holdoutFraction * 100}%. There are {len(theData)} records.\")\n",
    "    # Create holdout (20%)\n",
    "    holdout = theData.sample(frac=holdoutFraction, random_state=42)\n",
    "\n",
    "    # Create training (remaining 80%)\n",
    "    training = theData.drop(holdout.index)\n",
    "\n",
    "    # Optional: save to files\n",
    "    holdout.to_csv(\"data/holdout.csv\", index=False)\n",
    "    training.to_csv(\"data/training.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Exploration Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphCategoricalVsTarget(theData, targetValue, limit=None):\n",
    "    categoricalData = theData.select_dtypes(include='object').columns\n",
    "    if isinstance(limit, int) and limit > 0:\n",
    "        categoricalData = categoricalData[:limit]\n",
    "\n",
    "    for col in categoricalData:\n",
    "        if col.endswith('-orig'):\n",
    "            continue\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.barplot(x=targetValue, y=col, data=theData)\n",
    "        plt.title(f\"Our Target vs {col}\")\n",
    "        plt.xlabel('% Conversion')\n",
    "        plt.ylabel('')\n",
    "        plt.show()\n",
    "\n",
    "def graphNumericalVsTarget(theData, targetValue, limit=None):\n",
    "    numericalData = theData.select_dtypes(include=['int64', 'float64']).columns\n",
    "    if isinstance(limit, int) and limit > 0:\n",
    "        numericalData = numericalData[:limit]\n",
    "\n",
    "    for col in numericalData:\n",
    "        if col.endswith('-orig'):\n",
    "            continue\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.barplot(x=col, y=targetValue, data=theData)\n",
    "        plt.title(f\"Our Target vs {col}\")\n",
    "        plt.ylabel('% Conversion')\n",
    "        plt.xlabel('')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Exploration Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "theTrainingData = pd.read_csv(\"data/training.csv\")\n",
    "if performDataExploration:\n",
    "    print(f\"!!!!!: Performing data exploration\")\n",
    "    graphCategoricalVsTarget(theTrainingData, 'price')\n",
    "    #graphNumericalVsTarget(theTrainingData, 'price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select, Encode and Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyXGBoost(df, features, target, showHeader=True, test_size=0.2, random_state=42, n_estimators=250, max_depth=5, learning_rate=0.1):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df[features], df[target], test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    xgb_model = xgb.XGBRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        learning_rate=learning_rate,\n",
    "        random_state=random_state,\n",
    "#        reg_alpha=0.1,\n",
    "#        reg_lambda=1.0\n",
    "#        subsample=0.8,\n",
    "   #     colsample_bytree=0.8\n",
    "    )\n",
    "    xgb_model.fit(X_train,y_train)\n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "    xgb_r2 = r2_score(y_test, y_pred)\n",
    "    xgb_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "    feature_str = \", \".join(features)\n",
    "\n",
    "    if showHeader:\n",
    "        print(\"\\n====================================\")\n",
    "        print(f\"Features: {feature_str}\")\n",
    "        print(f\"Target: {target}\")\n",
    "        print(f\"Test Size: {test_size}\")\n",
    "        print(f\"Random State: {random_state}\")\n",
    "        print(f\"n_estimators: {n_estimators}\")\n",
    "        print(f\"max_depth: {max_depth}\")\n",
    "        print(f\"learning_rate: {learning_rate}\")\n",
    "        print(\"====================================\\n\")\n",
    "    print(f\"XGBoost R2 Score: {xgb_r2:.4f} | Root Mean Squared Error: {xgb_rmse:.4f}\")\n",
    "\n",
    "def applyXGBoostByFeature(df, feature_name, features, target, test_size=0.2, random_state=42, n_estimators=250, max_depth=5, learning_rate=0.1, threshold=0.4):\n",
    "  feature_vals = df[feature_name].unique()\n",
    "\n",
    "  for val in feature_vals:\n",
    "    df_val_instances = df[df[feature_name] == val]\n",
    "    record_count = len(df_val_instances)\n",
    "\n",
    "    print(f\"\\n=== Processing Feature Value: {val} | Records: {record_count} ===\")\n",
    "    applyXGBoost(df_val_instances, features, target, False, test_size, random_state, n_estimators, max_depth, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================\n",
      "Features: casting_b, casting_u, casting_g, casting_r, casting_w, casting_x, power, toughness, frequency, abilityAffinity, abilityAftermath, abilityAdventure, abilityCascade, abilityCompanion, abilityConvoke, abilityCycling, abilityCumulativeUpkeep, abilityDeathtouch, abilityDelve, abilityDefender, abilityDoubleStrike, abilityEcho, abilityEnchant, abilityEncore, abilityEquip, abilityEscalate, abilityEscape, abilityExalted, abilityExploit, abilityEvolve, abilityExtort, abilityFirstStrike, abilityFlash, abilityFlashback, abilityFlanking, abilityFlying, abilityForetell, abilityHaste, abilityHexproof, abilityIndestructible, abilityInfect, abilityIntimidate, abilityLandwalk, abilityLifelink, abilityMegamorph, abilityMenace, abilityMiracle, abilityMorph, abilityMutate, abilityPersist, abilityPhasing, abilityProtection, abilityProwess, abilityRampage, abilityReach, abilityScry, abilityShadow, abilityShroud, abilitySplitSecond, abilitySuspend, abilityToxic, abilityTrample, abilityUndying, abilityVigilance, daysSinceRelease, daysSinceGameRelease\n",
      "Target: price\n",
      "Test Size: 0.2\n",
      "Random State: 39\n",
      "n_estimators: 350\n",
      "max_depth: 13\n",
      "learning_rate: 0.14\n",
      "====================================\n",
      "\n",
      "XGBoost R2 Score: 0.5060 | Root Mean Squared Error: 2.3182\n"
     ]
    }
   ],
   "source": [
    "features = [\n",
    "        #'competitorName',\n",
    "        #'price', # this is our target variable\n",
    "        #'qty',\n",
    "        'casting_b',\n",
    "        'casting_u',\n",
    "        'casting_g',\n",
    "        'casting_r',\n",
    "        'casting_w',\n",
    "        # 'casting_bu',\n",
    "        # 'casting_bg',\n",
    "        # 'casting_br',\n",
    "        # 'casting_bw',\n",
    "        # 'casting_ug',\n",
    "        # 'casting_ur',\n",
    "        # 'casting_uw',\n",
    "        # 'casting_gr',\n",
    "        # 'casting_gw',\n",
    "        # 'casting_rw',\n",
    "        # 'casting_2b',\n",
    "        # 'casting_2u',\n",
    "        # 'casting_2g',\n",
    "        # 'casting_2r',\n",
    "        # 'casting_2w',\n",
    "        # 'casting_bp',\n",
    "        # 'casting_up',\n",
    "        # 'casting_gp',\n",
    "        # 'casting_rp',\n",
    "        # 'casting_wp',\n",
    "        #'casting_colorless',\n",
    "        'casting_x',\n",
    "        #'converted_casting_cost',\n",
    "        #'color', # Not needed as spelltype can be determined by the casting cost variables\n",
    "        #'spelltype',\n",
    "        'power',\n",
    "        'toughness',\n",
    "        'frequency',\n",
    "        'abilityAffinity',\n",
    "        'abilityAftermath',\n",
    "        'abilityAdventure',\n",
    "        'abilityCascade',\n",
    "        'abilityCompanion',\n",
    "        'abilityConvoke',\n",
    "        'abilityCycling',\n",
    "        'abilityCumulativeUpkeep',\n",
    "        'abilityDeathtouch',\n",
    "        'abilityDelve',\n",
    "        'abilityDefender',\n",
    "        'abilityDoubleStrike',\n",
    "        'abilityEcho',\n",
    "        'abilityEnchant',\n",
    "        'abilityEncore',\n",
    "        'abilityEquip',\n",
    "        'abilityEscalate',\n",
    "        'abilityEscape',\n",
    "        'abilityExalted',\n",
    "        'abilityExploit',\n",
    "        'abilityEvolve',\n",
    "        'abilityExtort',\n",
    "        'abilityFirstStrike',\n",
    "        'abilityFlash',\n",
    "        'abilityFlashback',\n",
    "        'abilityFlanking',\n",
    "        'abilityFlying',\n",
    "        'abilityForetell',\n",
    "        'abilityHaste',\n",
    "        'abilityHexproof',\n",
    "        'abilityIndestructible',\n",
    "        'abilityInfect',\n",
    "        'abilityIntimidate',\n",
    "        'abilityLandwalk',\n",
    "        'abilityLifelink',\n",
    "        'abilityMegamorph',\n",
    "        'abilityMenace',\n",
    "        'abilityMiracle',\n",
    "        'abilityMorph',\n",
    "        'abilityMutate',\n",
    "        'abilityPersist',\n",
    "        'abilityPhasing',\n",
    "        'abilityProtection',\n",
    "        'abilityProwess',\n",
    "        'abilityRampage',\n",
    "        'abilityReach',\n",
    "        'abilityScry',\n",
    "        'abilityShadow',\n",
    "        'abilityShroud',\n",
    "        'abilitySplitSecond',\n",
    "        'abilitySuspend',\n",
    "        'abilityToxic',\n",
    "        'abilityTrample',\n",
    "        'abilityUndying',\n",
    "        'abilityVigilance',\n",
    "        'daysSinceRelease',\n",
    "        'daysSinceGameRelease'\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "target = \"price\"\n",
    "#applyXGBoost(theTrainingData, features, \"price\", True, 0.2, 42, 250, 6, 0.05)\n",
    "#applyXGBoost(theTrainingData, features, \"price\", True, 0.2, 42, 500, 6, 0.1) #R2 Score: 0.3143 | Root Mean Squared Error: 2.7325\n",
    "#applyXGBoost(theTrainingData, features, \"price\", True, 0.2, 42, 1000, 6, 0.1) #R2 Score: 0.3463 | Root Mean Squared Error: 2.6681\n",
    "#applyXGBoost(theTrainingData, features, \"price\", True, 0.2, 42, 500, 6, 0.05) #R2 Score: 0.2841 | Root Mean Squared Error: 2.7922\n",
    "#applyXGBoost(theTrainingData, features, \"price\", True, 0.2, 42, 500, 6, 0.2) #R2 Score: 0.3485 | Root Mean Squared Error: 2.6636\n",
    "#applyXGBoost(theTrainingData, features, \"price\", True, 0.2, 42, 1000, 6, 0.2) #R2 Score: 0.3755 | Root Mean Squared Error: 2.6078\n",
    "#applyXGBoost(theTrainingData, features, \"price\", True, 0.2, 42, 1000, 6, 0.1) #R2 Score: 0.3463 | Root Mean Squared Error: 2.6681\n",
    "#applyXGBoost(theTrainingData, features, \"price\", True, 0.2, 42, 1500, 6, 0.2) #R2 Score: 0.3886 | Root Mean Squared Error: 2.5804\n",
    "#applyXGBoost(theTrainingData, features, \"price\", True, 0.2, 42, 1500, 6, 0.2) # All abilities - R2 Score: 0.4476 | Root Mean Squared Error: 2.4526\n",
    "#applyXGBoost(theTrainingData, features, \"price\", True, 0.2, 42, 1500, 6, 0.1) # All abilities - R2 Score: 0.4159 | Root Mean Squared Error: 2.5220\n",
    "#applyXGBoost(theTrainingData, features, \"price\", True, 0.2, 42, 1500, 8, 0.1) # All abilities R2 Score: 0.4686 | Root Mean Squared Error: 2.4056\n",
    "#applyXGBoost(theTrainingData, features, \"price\", True, 0.2, 42, 1500, 12, 0.1) # All abilities R2 Score: 0.4972 | Root Mean Squared Error: 2.3399\n",
    "#applyXGBoost(theTrainingData, features, \"price\", True, 0.2, 42, 2500, 8, 0.05) # All abilities R2 Score: 0.4584 | Root Mean Squared Error: 2.4285\n",
    "#applyXGBoost(theTrainingData, features, \"price\", True, 0.2, 42, 2500, 12, 0.05) # All abilities R2 Score: 0.5017 | Root Mean Squared Error: 2.3294\n",
    "#applyXGBoost(theTrainingData, features, \"price\", True, 0.2, 42, 10000, 12, 0.1) # All abilities 0.4174 | Root Mean Squared Error: 2.5189\n",
    "#applyXGBoost(theTrainingData, features, \"price\", True, 0.2, 42, 2500, 12, 0.1) # all abilities R2 Score: 0.4825 | Root Mean Squared Error: 2.3738\n",
    "#applyXGBoost(theTrainingData, features, \"price\", True, 0.2, 42, 10000, 12, 0.05) # R2 Score: 0.4583 | Root Mean Squared Error: 2.4288\n",
    "#applyXGBoost(theTrainingData, features, \"price\", True, 0.2, 42, 10000, 12, 0.1) # R2 Score: 0.4174 | Root Mean Squared Error: 2.5189\n",
    "#applyXGBoost(theTrainingData, features, \"price\", True, 0.2, 42, 20000, 12, 0.12) # R2 Score: 0.4000 | Root Mean Squared Error: 2.5562\n",
    "#applyXGBoost(theTrainingData, features, \"price\", True, 0.2, 42, 20000, 15, 0.15) # R2 Score: 0.4108 | Root Mean Squared Error: 2.5331\n",
    "#applyXGBoost(theTrainingData, features, \"price\", True, 0.2, 42, 20000, 15, 0.17) # R2 Score: 0.4108 | Root Mean Squared Error: 2.5331\n",
    "#applyXGBoost(theTrainingData, features, \"price\", True, 0.2, 42, 20000, 20, 0.17) # R2 Score: 0.4228 | Root Mean Squared Error: 2.5071\n",
    "applyXGBoost(theTrainingData, features, \"price\", True, 0.2, 39, 350, 13, 0.14)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
